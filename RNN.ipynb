{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "主にhttp://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "のノーテーションでみていこう。\n",
    "<img src = 'imgs/LSTM3-chain.png'>\n",
    "\n",
    "\n",
    "\n",
    "ただし、どの部分がどう呼ばれているかは、\n",
    "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "の図表がわかりやすい：\n",
    "\n",
    "<img src  = 'imgs/1*yBXV9o5q7L_CvY7quJt3WQ.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "多分インプット以外は全部同じdimension(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Candidate $C_n$は出力の一つ手前の隠れ層の役割を果たしている\n",
    "  - $h_n$は出力なのでそのひとつ手前の隠れ層と思えばよい？\n",
    "  - LSTMの模式図では一番奥に書いてあるが、$x_n \\rightarrow C \\rightarrow h$と並べたほうが自然（？）\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x_n$,$h_n(=y_n)$に依存する（$C_{n-1}$に依存しない）３つのgateがある (すべてsigmoidで終わる = forgettable)\n",
    "    - forget gate <img src = 'imgs/LSTM3-focus-f.png'>\n",
    "    - input gate<img src = 'imgs/LSTM3-focus-i.png'>\n",
    "    上図の$i_t$を出力とするのがinput gate\n",
    "    - output gate<img src = 'imgs/LSTM3-focus-o.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Candidateのバリューはひとつ前$C_{n-1}$から遺伝するが、forget gateを通してボリューム調整する\n",
    "- 忘却プロセスを経たCandidateをinput gate出力でボリューム調整した「$x_n$,$h_n(=y_n)$に依存する$\\tanh$出力」を加算することでアップデートする\n",
    "\n",
    "<img src = 'imgs/LSTM3-focus-C.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 上記いずれのプロセスも$C_{n-1}$自体はほとんど用いないことに注意\n",
    "- $C_n$が決定したら$\\tanh$をかけて、output gate出力でボリューム調整して$h_n$($n$ word目の出力）とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "hidden_size = 20\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 5\n",
    "sequence_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(sequence_length, sequence_num, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.randn(num_layers , sequence_num , hidden_size)\n",
    "c0 = torch.randn(num_layers , sequence_num, hidden_size)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight_ih_l0', torch.Size([80, 10])),\n",
       " ('weight_hh_l0', torch.Size([80, 20])),\n",
       " ('bias_ih_l0', torch.Size([80])),\n",
       " ('bias_hh_l0', torch.Size([80]))]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tuple([x[0] , x[1].shape]) for x in rnn.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n",
    "\n",
    "<img src = 'imgs/LSTM3-var-GRU.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Candidateがない。\n",
    "- このため、一層の場合、前回の出力$h_{n-1}$と今回の入力$x_n$のみから今回の出力$h_{n-1}$を決めることになる\n",
    "- これは結構厳しいのではないかという印象を受けるがワークするのはなぜなのだろう？\n",
    "    - LSTMに劣るという報告もあるようだが（要出典 To be added）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "hidden_size = 20\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.GRU(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 5\n",
    "sequence_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.randn(num_layers , sequence_num , hidden_size)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cell version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence長方向のひとつひとつの作用を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n",
    "cell state は存在しない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cell = nn.GRUCell(input_size = input_size , hidden_size = hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cell = torch.randn( sequence_num  , input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0_cell = torch.randn(sequence_num , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8335, -0.2871,  0.6537,  0.4636, -0.2017, -0.4652, -0.0595, -0.6599,\n",
       "         -0.9843,  0.1885,  0.3383, -0.6164, -1.0814,  0.1980,  0.9775,  0.3202,\n",
       "          0.5286,  0.5524,  0.2044,  0.1118],\n",
       "        [ 0.4493, -0.3635,  0.0778, -0.0202,  0.7757,  0.1915,  0.4439,  0.2207,\n",
       "          1.0404, -0.1683,  0.5032, -0.0317, -0.4123, -1.1217, -0.3295, -0.7662,\n",
       "         -0.5264, -0.2910,  0.8186,  0.5164],\n",
       "        [ 0.1806,  0.8379,  1.2413, -0.4384, -0.3630, -0.3390,  0.1768, -0.2117,\n",
       "         -1.3939, -0.1366,  0.6929, -0.2663, -0.5033,  0.3618,  0.2595, -0.0419,\n",
       "          0.5714,  0.5075, -0.7026,  0.7079]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_cell(input_cell , h0_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cell = nn.LSTMCell(input_size=input_size , hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0_cell = torch.randn(sequence_num , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-68b29ef937f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_cell\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0_cell\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mc0_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/base2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-eed340abb210>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mi_gate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_gate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_gate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_gate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# chunkは第二引数方向に第一引数分割\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mi_gate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_gate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mf_gate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_gate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mc_gate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_gate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "rnn_cell(input_cell , (h0_cell , c0_cell))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/49040180/change-tanh-activation-in-lstm-to-relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, nlayers, dropout):\n",
    "        \"\"\"\"Constructor of the class\"\"\"\n",
    "        super(my_LSTMCell, self).__init__()#input_size , hidden_size)\n",
    "        #super().__init__()#input_size, hidden_size)\n",
    "\n",
    "        self.nlayers = nlayers\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        ih, hh = [], []\n",
    "        for i in range(nlayers):\n",
    "            ih.append(nn.Linear(input_size, 4 * hidden_size))\n",
    "            hh.append(nn.Linear(hidden_size, 4 * hidden_size))\n",
    "        self.w_ih = nn.ModuleList(ih)\n",
    "        self.w_hh = nn.ModuleList(hh)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"\"Defines the forward computation of the LSTMCell\"\"\"\n",
    "        hy, cy = [], []\n",
    "        for i in range(self.nlayers):\n",
    "            hx, cx = hidden[0][i], hidden[1][i]\n",
    "            gates = self.w_ih[i](input) + self.w_hh[i](hx)\n",
    "            i_gate, f_gate, c_gate, o_gate = gates.chunk(4, 1) # chunkは第二引数方向に第一引数分割\n",
    "\n",
    "            i_gate = torch.sigmoid(i_gate)\n",
    "            f_gate = torch.sigmoid(f_gate)\n",
    "            c_gate = F.tanh(c_gate)\n",
    "            o_gate = torch.sigmoid(o_gate)\n",
    "\n",
    "            ncx = (f_gate * cx) + (i_gate * c_gate)\n",
    "            nhx = o_gate * F.tanh(ncx)\n",
    "            cy.append(ncx)\n",
    "            hy.append(nhx)\n",
    "            input = self.dropout(nhx)\n",
    "\n",
    "        hy, cy = torch.stack(hy, 0), torch.stack(cy, 0)\n",
    "        return hy, cy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chunkは第二引数方向に第一引数分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cell = my_LSTMCell(input_size=input_size , hidden_size=hidden_size , nlayers=1 , dropout = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0153, -0.0195,  0.0217, -0.1277, -0.1335,  0.6000,  0.0666,\n",
       "           -0.1095, -0.0361,  0.2820, -0.3620,  0.4399, -0.2040,  0.1220,\n",
       "           -0.0065, -0.0982, -0.3041,  0.2062,  0.1905, -0.5630],\n",
       "          [ 0.1604, -0.2095,  0.2442,  0.0333,  0.0181,  0.2809, -0.0675,\n",
       "           -0.1081,  0.0338,  0.2612, -0.3391,  0.4058, -0.1925,  0.0652,\n",
       "            0.0232,  0.1189, -0.0761, -0.6512,  0.1925, -0.3242],\n",
       "          [-0.0127, -0.1873,  0.0176, -0.0264, -0.0349,  0.3741,  0.0855,\n",
       "           -0.2058, -0.0437,  0.4875, -0.3991,  0.4650, -0.4058,  0.1545,\n",
       "            0.0276,  0.0218,  0.0465,  0.0267,  0.4557, -0.4281]]],\n",
       "        grad_fn=<StackBackward>),\n",
       " tensor([[[ 0.0425, -0.1363,  0.1183, -0.8400, -0.3843,  0.9999,  0.0893,\n",
       "           -0.9688, -0.0657,  0.8581, -0.5932,  0.7532, -0.3199,  0.1576,\n",
       "           -0.0134, -0.3333, -0.4158,  0.3860,  0.6042, -0.9542],\n",
       "          [ 0.2280, -0.3540,  0.5198,  0.0738,  0.1099,  0.7289, -0.1476,\n",
       "           -0.4229,  0.0836,  0.3455, -0.7793,  0.4701, -1.4664,  0.1114,\n",
       "            0.0669,  0.2677, -0.1259, -0.9587,  0.5291, -0.6343],\n",
       "          [-0.0228, -0.6827,  0.0690, -0.0486, -0.2344,  0.7661,  0.1165,\n",
       "           -0.6166, -0.1460,  0.9592, -0.5595,  0.6940, -1.0415,  0.1916,\n",
       "            0.0640,  0.0566,  0.1597,  0.0345,  0.7671, -0.7228]]],\n",
       "        grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_cell(input_cell , (h0_cell , c0_cell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
