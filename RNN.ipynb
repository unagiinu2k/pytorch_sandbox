{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "主にこちらのノーテーションをみていこう。http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "<img src = 'imgs/LSTM3-chain.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Candidate $C_n$は出力の一つ手前の隠れ層の役割を果たしている\n",
    "  - $h_n$は出力なのでそのひとつ手前の隠れ層と思えばよい？\n",
    "  - LSTMの模式図では一番奥に書いてあるが、$x_n \\rightarrow C \\rightarrow h$と並べたほうが自然（？）\n",
    "- $x_n$,$h_n(=y_n)$に依存する（$C_{n-1}$に依存しない）３つのgateがある (すべてsigmoidで終わる = forgettable)\n",
    "    - forget gate <img src = 'imgs/LSTM3-focus-f.png'>\n",
    "    - input gate<img src = 'imgs/LSTM3-focus-i.png'>\n",
    "    上図の$i_t$を出力とするのがinput gate\n",
    "    - output gate\n",
    "\n",
    "- Candidateのバリューはひとつ前$C_{n-1}$から遺伝するが、forget gateを通してボリューム調整する\n",
    "- 忘却プロセスを経たCandidateをinput gate出力でボリューム調整した「$x_n$,$h_n(=y_n)$に依存する$\\tanh$出力」を加算することでアップデートする\n",
    "- 上記いずれのプロセスも$C_{n-1}$自体はほとんど用いないことに注意\n",
    "- $C_n$が決定したら$\\tanh$をかけて、output gate出力でボリューム調整して$h_n$($n$ word目の出力）とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "hidden_size = 20\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 5\n",
    "sequence_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(sequence_length, sequence_num, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.randn(num_layers , sequence_num , hidden_size)\n",
    "c0 = torch.randn(num_layers , sequence_num, hidden_size)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n",
    "- Candidateがない。\n",
    "- このため、一層の場合、前回の出力$h_{n-1}$と今回の入力$x_n$のみから今回の出力$h_{n-1}$を決めることになる\n",
    "- これは結構厳しいのではないかという印象を受けるがワークするのはなぜなのだろう？\n",
    "    - LSTMに劣るという報告もあるようだが（要出典 To be added）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "hidden_size = 20\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.GRU(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 5\n",
    "sequence_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.randn(num_layers , sequence_num , hidden_size)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cell version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence長方向のひとつひとつの作用を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n",
    "cell state は存在しない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cell = nn.GRUCell(input_size = input_size , hidden_size = hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cell = torch.randn( sequence_num  , input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0_cell = torch.randn(sequence_num , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8335, -0.2871,  0.6537,  0.4636, -0.2017, -0.4652, -0.0595, -0.6599,\n",
       "         -0.9843,  0.1885,  0.3383, -0.6164, -1.0814,  0.1980,  0.9775,  0.3202,\n",
       "          0.5286,  0.5524,  0.2044,  0.1118],\n",
       "        [ 0.4493, -0.3635,  0.0778, -0.0202,  0.7757,  0.1915,  0.4439,  0.2207,\n",
       "          1.0404, -0.1683,  0.5032, -0.0317, -0.4123, -1.1217, -0.3295, -0.7662,\n",
       "         -0.5264, -0.2910,  0.8186,  0.5164],\n",
       "        [ 0.1806,  0.8379,  1.2413, -0.4384, -0.3630, -0.3390,  0.1768, -0.2117,\n",
       "         -1.3939, -0.1366,  0.6929, -0.2663, -0.5033,  0.3618,  0.2595, -0.0419,\n",
       "          0.5714,  0.5075, -0.7026,  0.7079]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_cell(input_cell , h0_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cell = nn.LSTMCell(input_size=input_size , hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/49040180/change-tanh-activation-in-lstm-to-relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, nlayers, dropout):\n",
    "        \"\"\"\"Constructor of the class\"\"\"\n",
    "        super(LSTMCell, self).__init__()\n",
    "\n",
    "        self.nlayers = nlayers\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        ih, hh = [], []\n",
    "        for i in range(nlayers):\n",
    "            ih.append(nn.Linear(input_size, 4 * hidden_size))\n",
    "            hh.append(nn.Linear(hidden_size, 4 * hidden_size))\n",
    "        self.w_ih = nn.ModuleList(ih)\n",
    "        self.w_hh = nn.ModuleList(hh)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"\"Defines the forward computation of the LSTMCell\"\"\"\n",
    "        hy, cy = [], []\n",
    "        for i in range(self.nlayers):\n",
    "            hx, cx = hidden[0][i], hidden[1][i]\n",
    "            gates = self.w_ih[i](input) + self.w_hh[i](hx)\n",
    "            i_gate, f_gate, c_gate, o_gate = gates.chunk(4, 1)\n",
    "\n",
    "            i_gate = F.sigmoid(i_gate)\n",
    "            f_gate = F.sigmoid(f_gate)\n",
    "            c_gate = F.tanh(c_gate)\n",
    "            o_gate = F.sigmoid(o_gate)\n",
    "\n",
    "            ncx = (f_gate * cx) + (i_gate * c_gate)\n",
    "            nhx = o_gate * F.tanh(ncx)\n",
    "            cy.append(ncx)\n",
    "            hy.append(nhx)\n",
    "            input = self.dropout(nhx)\n",
    "\n",
    "        hy, cy = torch.stack(hy, 0), torch.stack(cy, 0)\n",
    "        return hy, cy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
