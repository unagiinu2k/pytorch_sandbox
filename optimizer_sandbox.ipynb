{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: %%bash is a cell magic, but the cell body is empty.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch gradient descent\n",
    "\n",
    "普通の勾配法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stochcastic gradient descent\n",
    "\n",
    "一サンプル毎の勾配法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mini-batch gradient descent\n",
    "\n",
    "50サンプル毎とかのmini batch毎の勾配法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# momentum\n",
    "\n",
    "- 鋭角なハーフパイプのような斜面のボトム以外に目隠しをして立っているイメージ\n",
    "- ハーフパイプのゴールに向けて進むように前回のアップデートからの寄与を加える\n",
    "- learning rateを変えなければゴール方向へ進むスピードは速くならないが、同じlearning rateだと更新幅が小さくなるのでlearning rateを上げることができる（？）、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nesterov accelerated descent (NAG)\n",
    "\n",
    "momentumの工夫に加えて、勾配を計算する地点を\n",
    "```\n",
    "現在地から前回更新の半分（例）だけ更新した地点\n",
    "```\n",
    "とする\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adagrad\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta_{t+1, i } = \\theta_{t,i} -\\frac{\\eta}{\\sqrt{G_{t,ii}+\\epsilon}}\\nabla_i J(\\theta_t)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "$$\n",
    "G_{t,ii}= \\sum_{u=0}^{t} \\left(\\nabla_iJ(\\theta_u)\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習が非常に遅くなってしまうのが問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adadelta\n",
    "\n",
    "- Adagradの$G_{t,ii}$を$t=0$からの和ではなく、一定長のmoving averageとするイメージ \n",
    "- 実際にはmoving averageではなく、\"exponentially decaying average\"\n",
    "\n",
    "このあたり参照URLのノーテーションが若干疑問"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSprop\n",
    "\n",
    "後回し"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam (Adaptive Moment Estimation )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 勾配の二乗のexponentially decaying averageだけでなく\n",
    "- 勾配自身のexponentially decaying averageもstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-　更新が小さくなりがちなので補正を入れる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adamax\n",
    "\n",
    "二乗ノルムを他のノルムに拡張したもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nadam\n",
    "\n",
    "Nesterov的な工夫をAdamに導入したもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMSGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adaptiveな手法はしばしばmomentum法に負ける\n",
    "- Reddi(2018)はexponential moving averageが原因であると指摘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- moving averageの代わりに前回までとのmax演算に変更\n",
    "- bias補正もやめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base2]",
   "language": "python",
   "name": "conda-env-base2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
